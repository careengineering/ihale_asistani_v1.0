

===== src\data_preprocessing.py =====
import os
import zipfile
import tempfile
from pathlib import Path
import PyPDF2
from docx import Document
from bs4 import BeautifulSoup
import re
import json
from collections import defaultdict

def extract_text_from_pdf(file_path):
    """PDF dosyasından metin çıkarır, UTF-8 kodlamasını korur."""
    try:
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ''
            for page in reader.pages:
                page_text = page.extract_text() or ''
                text += page_text
        return text.encode('utf-8').decode('utf-8', errors='ignore')
    except Exception as e:
        print(f"Hata: {file_path} işlenemedi. {e}")
        return ''

def extract_text_from_docx(file_path):
    """DOCX dosyasından metin çıkarır, UTF-8 kodlamasını korur."""
    try:
        doc = Document(file_path)
        text = '\n'.join([para.text for para in doc.paragraphs])
        return text.encode('utf-8').decode('utf-8', errors='ignore')
    except Exception as e:
        print(f"Hata: {file_path} işlenemedi. {e}")
        return ''

def extract_text_from_html(file_path):
    """HTML dosyasından metin çıkarır, UTF-8 kodlamasını korur."""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            soup = BeautifulSoup(file, 'html.parser')
            text = soup.get_text(separator='\n', strip=True)
        return text.encode('utf-8').decode('utf-8', errors='ignore')
    except Exception as e:
        print(f"Hata: {file_path} işlenemedi. {e}")
        return ''

def extract_text_from_txt(file_path):
    """TXT dosyasından metin çıkarır, UTF-8 kodlamasını korur."""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
        return text.encode('utf-8').decode('utf-8', errors='ignore')
    except Exception as e:
        print(f"Hata: {file_path} işlenemedi. {e}")
        return ''

def extract_text_from_file(file_path):
    """Dosya uzantısına göre uygun metin çıkarma fonksiyonunu çağırır."""
    ext = Path(file_path).suffix.lower()
    if ext == '.pdf':
        return extract_text_from_pdf(file_path)
    elif ext == '.docx':
        return extract_text_from_docx(file_path)
    elif ext == '.html':
        return extract_text_from_html(file_path)
    elif ext == '.txt':
        return extract_text_from_txt(file_path)
    else:
        print(f"Desteklenmeyen dosya türü: {file_path}")
        return ''

def unzip_file(zip_path, extract_to):
    """ZIP dosyasını belirtilen klasöre çıkarır."""
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
        return extract_to
    except Exception as e:
        print(f"Hata: {zip_path} açılamadı. {e}")
        return None

def clean_text(text, is_kik=False):
    """Metni temizler: teknik şablonları, gereksiz bilgileri ve bozuk karakterleri kaldırır."""
    if not text:
        return ''
    
    # Bozuk karakterleri temizle, Türkçe karakterleri koru
    text = re.sub(r'[^\w\s.,;:-çğışöüÇĞİŞÖÜ]', ' ', text)
    
    # Yürütme/yürürlük maddelerinden sonrasını kaldır
    text = re.sub(r'(Yürütme|Yürürlük)\s+Madde.*?(?=\Z)', '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # Teknik şablonları ve gereksiz bilgileri kaldır
    patterns = [
        r'Telefon numarası:.*?(?=(Madde|\Z))',
        r'Faks numarası:.*?(?=(Madde|\Z))',
        r'\.{3,}\s*',
        r'\b(Mülga|Değişik ibare|yürürlük):?\s*\d{1,2}/\d{1,2}/\d{4}\s*RG\d*\.?\s*md\.?',
        r'İhale kayıt numarası:.*?(?=(Madde|\Z))',
        r'\b(Şartname|EK-\d+|Standart Form|Tutanağı|Tutanagi).*?(?=(Madde|\Z))',
        r'\|\s*\|+',  # Tablo benzeri yapılar
        r'\[.*?\]',   # Köşeli parantezler
    ]
    for pattern in patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # KİK kararları için gereksiz bilgileri kaldır
    if is_kik:
        text = re.sub(r'(Toplantı No|Gündem No|Katılan Üye Sayısı):.*?(?=(Gündem Konusu|\Z))', '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # Fazla boşlukları temizle, Türkçe karakterleri koru
    text = re.sub(r'\s+', ' ', text.strip())
    text = re.sub(r'([a-zçğışöü])([A-ZÇĞİŞÖÜ])', r'\1 \2', text)
    if len(text) < 30 or text.isspace():
        return ''
    return text

def chunk_text(text, max_length=500):
    """Metni anlamlı parçalara böler, Türkçe karakterleri korur."""
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-ZÇĞİŞÖÜ][a-zçğışöü]\.)(?<=\.|\?)\s', text)
    chunks = []
    current_chunk = ''
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) < max_length:
            current_chunk += ' ' + sentence
        else:
            cleaned_chunk = clean_text(current_chunk)
            if cleaned_chunk:
                chunks.append(cleaned_chunk)
            current_chunk = sentence
    cleaned_chunk = clean_text(current_chunk)
    if cleaned_chunk:
        chunks.append(cleaned_chunk)
    
    return chunks

def get_category(file_path):
    """Dosyanın kategorisini belirler."""
    file_path = str(file_path).lower()
    if 'genel şartname' in file_path:
        return 'genel_sartname'
    elif 'kanun' in file_path:
        return 'kanun'
    elif 'yonetmelik' in file_path:
        return 'yonetmelik'
    elif 'teblig' in file_path:
        return 'teblig'
    elif 'esaslar' in file_path:
        return 'esaslar'
    elif 'kik_kararlari' in file_path:
        return 'kik_kararlari'
    return 'bilinmeyen'

def get_ihale_type(file_path):
    """Dosyanın ihale türünü belirler."""
    file_path = str(file_path).lower()
    if 'mal' in file_path:
        return 'Mal'
    elif 'hizmet' in file_path:
        return 'Hizmet'
    elif 'yapım' in file_path or 'yapim' in file_path:
        return 'Yapım'
    elif 'danışmanlık' in file_path or 'danismanlik' in file_path:
        return 'Danışmanlık'
    return 'Genel'

def get_mevzuat_name(file_path):
    """Dosyanın mevzuat ismini daha spesifik şekilde çıkarır."""
    file_path = str(file_path).lower()
    file_name = Path(file_path).stem.replace('_', ' ')
    
    keywords = {
        'kamu ihale kanunu': 'Kamu İhale Kanunu',
        'mal alımı ihaleleri uygulama yönetmeliği': 'Mal Alımı İhaleleri Uygulama Yönetmeliği',
        'hizmet alımı ihaleleri uygulama yönetmeliği': 'Hizmet Alımı İhaleleri Uygulama Yönetmeliği',
        'yapım işleri ihaleleri uygulama yönetmeliği': 'Yapım İşleri İhaleleri Uygulama Yönetmeliği',
        'danışmanlık hizmet alımları yönetmeliği': 'Danışmanlık Hizmet Alımları Yönetmeliği',
        'genel şartname': 'Genel Şartname',
        'tebliğ': 'Tebliğ',
        'esaslar': 'Esaslar',
        'kik karar': 'KİK Kararları'
    }
    for key, value in keywords.items():
        if key in file_name:
            return value
    return file_name.title()

def get_kik_decision_info(text):
    """KİK kararlarından tarih ve numara çıkarır."""
    date_pattern = r'(\d{1,2}\.\d{1,2}\.\d{4})'
    number_pattern = r'Karar No\s*:\s*(\d+/\d+|\d+-\w+\.\w+-\d+)'
    date_match = re.search(date_pattern, text)
    number_match = re.search(number_pattern, text)
    date = date_match.group(1) if date_match else 'Bilinmeyen Tarih'
    number = number_match.group(1) if number_match else 'Bilinmeyen Numara'
    return date, number

def process_files(raw_dir, processed_dir):
    """Tüm dosyaları tarar, metinleri çıkarır ve JSON olarak kaydeder."""
    data = []
    seen_texts = defaultdict(set)
    raw_path = Path(raw_dir)
    temp_dir = tempfile.mkdtemp()

    for root, _, files in os.walk(raw_path):
        for file in files:
            file_path = Path(root) / file
            file_name_lower = file.lower()
            if any(keyword in file_name_lower for keyword in ['ek-', 'standart form', 'tutanağı', 'tutanagi']):
                print(f"Atlandı: {file_path} (EK, Standart Form veya Tutanak)")
                continue
            category = get_category(file_path)
            ihale_type = get_ihale_type(file_path)
            mevzuat_name = get_mevzuat_name(file_path)
            is_kik = category == 'kik_kararlari'
            
            if file_path.suffix.lower() == '.zip':
                extract_path = unzip_file(file_path, Path(temp_dir) / file_path.stem)
                if extract_path:
                    for sub_root, _, sub_files in os.walk(extract_path):
                        for sub_file in sub_files:
                            sub_file_lower = sub_file.lower()
                            if any(keyword in sub_file_lower for keyword in ['ek-', 'standart form', 'tutanağı', 'tutanagi']):
                                print(f"Atlandı: {sub_file} (EK, Standart Form veya Tutanak)")
                                continue
                            sub_file_path = Path(sub_root) / sub_file
                            text = extract_text_from_file(sub_file_path)
                            if text:
                                chunks = chunk_text(clean_text(text, is_kik=is_kik))
                                for i, chunk in enumerate(chunks):
                                    if chunk not in seen_texts[sub_file_path.stem]:
                                        seen_texts[sub_file_path.stem].add(chunk)
                                        chunk_data = {
                                            'mevzuat_name': get_mevzuat_name(sub_file_path),
                                            'chunk_id': f"{sub_file_path.stem}_{i}",
                                            'text': chunk,
                                            'category': category,
                                            'ihale_type': ihale_type
                                        }
                                        if is_kik:
                                            date, number = get_kik_decision_info(text)
                                            chunk_data['kik_date'] = date
                                            chunk_data['kik_number'] = number
                                        data.append(chunk_data)
            else:
                text = extract_text_from_file(file_path)
                if text:
                    chunks = chunk_text(clean_text(text, is_kik=is_kik))
                    for i, chunk in enumerate(chunks):
                        if chunk not in seen_texts[file_path.stem]:
                            seen_texts[file_path.stem].add(chunk)
                            chunk_data = {
                                'mevzuat_name': mevzuat_name,
                                'chunk_id': f"{file_path.stem}_{i}",
                                'text': chunk,
                                'category': category,
                                'ihale_type': ihale_type
                            }
                            if is_kik:
                                date, number = get_kik_decision_info(text)
                                chunk_data['kik_date'] = date
                                chunk_data['kik_number'] = number
                            data.append(chunk_data)

    output_path = Path(processed_dir) / 'mevzuat_chunks.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"İşlenen veriler {output_path}'a kaydedildi. Toplam {len(data)} parça.")

if __name__ == '__main__':
    RAW_DIR = 'data/raw'
    PROCESSED_DIR = 'data/processed'
    os.makedirs(PROCESSED_DIR, exist_ok=True)
    process_files(RAW_DIR, PROCESSED_DIR)

===== src\embedder.py =====
import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import re
import torch
import pandas as pd

class Retriever:
    def __init__(self, index_path, metadata_path, model_name="distiluse-base-multilingual-cased-v2", synonym_file="data/EsAnlamlilar.csv"):
        self.model = SentenceTransformer(model_name, device="cuda" if torch.cuda.is_available() else "cpu")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Retriever Device: {self.device}")  # Cihazı doğrulamak için
        self.index = faiss.read_index(index_path)
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        faiss.omp_set_num_threads(4)

        # Eşanlamlılar dosyasını yükle (opsiyonel)
        self.synonym_dict = self.load_synonyms(synonym_file) if synonym_file else {}

    def load_synonyms(self, synonym_file):
        """Eşanlamlılar dosyasını okur ve bir sözlük oluşturur."""
        try:
            df = pd.read_csv(synonym_file, encoding='utf-8')
            # Sütun isimlerini kontrol et ve uyarla
            if 'Kelime' in df.columns and 'Eşanlamlılar' in df.columns:
                word_col, syn_col = 'Kelime', 'Eşanlamlılar'
            elif 'word' in df.columns and 'synonyms' in df.columns:
                word_col, syn_col = 'word', 'synonyms'
            else:
                raise ValueError("CSV dosyasında 'Kelime' ve 'Eşanlamlılar' veya 'word' ve 'synonyms' sütunları bulunamadı.")
            
            synonym_dict = {}
            for _, row in df.iterrows():
                word = row[word_col].strip().lower()
                synonyms = [syn.strip().lower() for syn in row[syn_col].split(',')]
                synonym_dict[word] = synonyms
                # Her eşanlamlıyı da ana kelimeye bağla (çift yönlü eşleşme)
                for syn in synonyms:
                    if syn not in synonym_dict:
                        synonym_dict[syn] = [word] + [s for s in synonyms if s != syn]
            print(f"Eşanlamlılar yüklendi: {len(synonym_dict)} kelime.")
            return synonym_dict
        except Exception as e:
            print(f"Hata: Eşanlamlılar dosyası yüklenemedi. {e}")
            return {}

    def expand_query(self, query):
        """Sorgudaki kelimeleri eşanlamlılarıyla genişletir."""
        if not self.synonym_dict:
            return query  # Eşanlamlılar yoksa orijinal sorguyu döndür
        words = query.lower().split()
        expanded_words = []
        for word in words:
            if word in self.synonym_dict:
                expanded_words.extend(self.synonym_dict[word])
            expanded_words.append(word)
        expanded_query = " ".join(set(expanded_words))  # Tekrarları kaldır
        print(f"Genişletilmiş sorgu: {expanded_query}")
        return expanded_query

    def clean_text(self, text):
        text = re.sub(r'[^\w\s.,;:-çğışöüÇĞİŞÖÜ]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def retrieve(self, query, top_k=5, max_distance=0.7, ihale_type="Genel"):
        try:
            # Sorguyu eşanlamlılarla genişlet
            expanded_query = self.expand_query(query)
            query_embedding = self.model.encode([self.clean_text(expanded_query)], convert_to_tensor=False)
            distances, indices = self.index.search(query_embedding, top_k)
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if distance > max_distance:
                    continue
                metadata = self.metadata[idx]
                if ihale_type != "Genel" and metadata.get("ihale_type") != ihale_type:
                    continue
                results.append({
                    "text": self.clean_text(metadata["text"]),
                    "mevzuat_name": metadata["mevzuat_name"],
                    "distance": float(distance),
                    "ihale_type": metadata.get("ihale_type", "Genel"),
                    "kik_date": metadata.get("kik_date"),
                    "kik_number": metadata.get("kik_number")
                })
            return results[:top_k]
        except Exception as e:
            print(f"Hata: Arama yapılamadı. {e}")
            return []

===== src\model.py =====
import torch
from transformers import pipeline

class ResponseGenerator:
    def __init__(self, model_name="savasy/bert-base-turkish-squad"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.qa_pipeline = pipeline(
            "question-answering",
            model=model_name,
            tokenizer=model_name,
            device=0 if self.device.type == "cuda" else -1
        )

    def generate_answer(self, question, context, max_context_length=512):
        try:
            context = context[:max_context_length]
            result = self.qa_pipeline({
                "question": question,
                "context": context
            })
            answer = result["answer"].strip()
            if not answer or len(answer) < 3:
                return "Üzgünüz, soruya uygun bir cevap bulunamadı."
            return f"{answer.capitalize()}."
        except Exception as e:
            print(f"Hata: Cevap üretilemedi. {e}")
            return "Üzgünüz, soruya uygun bir cevap üretilemedi."

===== src\retriever.py =====
import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import re
import torch
import pandas as pd

class Retriever:
    def __init__(self, index_path, metadata_path, model_name="distiluse-base-multilingual-cased-v2", synonym_file="data/EsAnlamlilar.csv"):
        self.model = SentenceTransformer(model_name, device="cuda" if torch.cuda.is_available() else "cpu")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Retriever Device: {self.device}")  # Cihazı doğrulamak için
        self.index = faiss.read_index(index_path)
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        faiss.omp_set_num_threads(4)

        # Eşanlamlılar dosyasını yükle
        self.synonym_dict = self.load_synonyms(synonym_file)

    def load_synonyms(self, synonym_file):
        """Eşanlamlılar dosyasını okur ve bir sözlük oluşturur."""
        try:
            df = pd.read_csv(synonym_file, encoding='utf-8')
            synonym_dict = {}
            for _, row in df.iterrows():
                word = row['Kelime'].strip().lower()
                synonyms = [syn.strip().lower() for syn in row['Eşanlamlılar'].split(',')]
                synonym_dict[word] = synonyms
                # Her eşanlamlıyı da ana kelimeye bağla (çift yönlü eşleşme)
                for syn in synonyms:
                    if syn not in synonym_dict:
                        synonym_dict[syn] = [word] + [s for s in synonyms if s != syn]
            print(f"Eşanlamlılar yüklendi: {len(synonym_dict)} kelime.")
            return synonym_dict
        except Exception as e:
            print(f"Hata: Eşanlamlılar dosyası yüklenemedi. {e}")
            return {}

    def expand_query(self, query):
        """Sorgudaki kelimeleri eşanlamlılarıyla genişletir."""
        words = query.lower().split()
        expanded_words = []
        for word in words:
            if word in self.synonym_dict:
                expanded_words.extend(self.synonym_dict[word])
            expanded_words.append(word)
        expanded_query = " ".join(set(expanded_words))  # Tekrarları kaldır
        print(f"Genişletilmiş sorgu: {expanded_query}")
        return expanded_query

    def clean_text(self, text):
        text = re.sub(r'[^\w\s.,;:-çğışöüÇĞİŞÖÜ]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def retrieve(self, query, top_k=5, max_distance=0.7, ihale_type="Genel"):
        try:
            # Sorguyu eşanlamlılarla genişlet
            expanded_query = self.expand_query(query)
            query_embedding = self.model.encode([self.clean_text(expanded_query)], convert_to_tensor=False)
            distances, indices = self.index.search(query_embedding, top_k)
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if distance > max_distance:
                    continue
                metadata = self.metadata[idx]
                if ihale_type != "Genel" and metadata.get("ihale_type") != ihale_type:
                    continue
                results.append({
                    "text": self.clean_text(metadata["text"]),
                    "mevzuat_name": metadata["mevzuat_name"],
                    "distance": float(distance),
                    "ihale_type": metadata.get("ihale_type", "Genel"),
                    "kik_date": metadata.get("kik_date"),
                    "kik_number": metadata.get("kik_number")
                })
            return results[:top_k]
        except Exception as e:
            print(f"Hata: Arama yapılamadı. {e}")
            return []

===== app\main.py =====
import streamlit as st
import sys
import os

# src dizinini Python yoluna ekle
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.retriever import Retriever
from src.model import ResponseGenerator

# Streamlit sayfa yapılandırması
st.set_page_config(page_title="İhale Asistanı", layout="wide")

# CSS ile arayüzü düzenleme
st.markdown("""
<style>
    .stApp { background-color: #f5f5f5; }
    .stButton>button { background-color: #4CAF50; color: white; }
    .stTextInput>div>input { border-radius: 5px; }
    h1 { color: #2E7D32; }
</style>
""", unsafe_allow_html=True)

# Modelleri başlatma
@st.cache_resource
def initialize_models():
    retriever = Retriever(
        index_path="data/embeddings/mevzuat_embeddings.faiss",
        metadata_path="data/embeddings/mevzuat_metadata.json",
        model_name="distiluse-base-multilingual-cased-v2",
        synonym_file="data/EsAnlamlilar.csv"
    )
    generator = ResponseGenerator(model_name="savasy/bert-base-turkish-squad")
    return retriever, generator

st.title("İhale Asistanı")
st.markdown("İhale süreçleriyle ilgili sorularınızı yanıtlayan bir asistan.")

# Model yükleme
with st.spinner("Modeller yükleniyor..."):
    retriever, generator = initialize_models()

# Sekmeler
tab1, tab2 = st.tabs(["Soru Sor", "Geçmiş Etkileşimler"])

# Soru sorma sekmesi
with tab1:
    st.subheader("Soru Sor")
    st.write("İhale türünü seçerek sorunuzun daha iyi filtrelenmesini sağlayabilirsiniz.")

    # Form ile soru girişi
    with st.form(key="question_form"):
        ihale_type = st.selectbox(
            "İhale türünü seçin:",
            ["Genel", "Mal", "Hizmet", "Yapım", "Danışmanlık"]
        )
        query = st.text_input(
            "İhale ile ilgili sorunuzu girin (örn: 'Diploma iş deneyim belgesi yerine geçer mi?'):",
            key="query_input"
        )
        submit_button = st.form_submit_button(label="Soruyu Gönder")

        if submit_button or query:
            if query.strip() == "":
                st.warning("Lütfen bir soru girin.")
            else:
                with st.spinner("Sorgunuz işleniyor..."):
                    # Retriever ile ilgili metinleri al
                    results = retriever.retrieve(query, top_k=5, max_distance=0.7, ihale_type=ihale_type)
                    
                    if not results:
                        st.error("Üzgünüz, sorunuzla ilgili yeterli bilgi bulunamadı.")
                    else:
                        # ResponseGenerator ile cevap üret
                        context = "\n".join([res["text"] for res in results])
                        answer = generator.generate_answer(query, context)
                        
                        # Cevabı ve kaynakları göster
                        st.subheader("Mevzuat Tabanlı Yanıt")
                        st.write(f"**Yanıt:** {answer}")
                        
                        st.subheader("Kaynak Mevzuat Maddeleri")
                        for res in results:
                            st.write(f"- **{res['mevzuat_name']}** (Mesafe: {res['distance']:.4f})")
                            if "kik_date" in res and "kik_number" in res:
                                st.write(f"  - Karar No: {res['kik_number']}, Tarih: {res['kik_date']}")
                            st.write(f"  - Metin: {res['text'][:200]}...")
                        
                        # Geçmişi kaydet
                        if "history" not in st.session_state:
                            st.session_state.history = []
                        st.session_state.history.append({
                            "question": query,
                            "answer": answer,
                            "ihale_type": ihale_type,
                            "sources": results
                        })

# Geçmiş etkileşimler sekmesi
with tab2:
    st.subheader("Geçmiş Etkileşimler")
    if "history" not in st.session_state or not st.session_state.history:
        st.write("Henüz kaydedilmiş etkileşim yok.")
    else:
        for i, entry in enumerate(st.session_state.history):
            st.write(f"**Soru {i+1}:** {entry['question']}")
            st.write(f"**İhale Türü:** {entry['ihale_type']}")
            st.write(f"**Yanıt:** {entry['answer']}")
            st.write("**Kaynaklar:**")
            for src in entry["sources"]:
                st.write(f"- {src['mevzuat_name']} (Mesafe: {src['distance']:.4f})")
            st.write("---")